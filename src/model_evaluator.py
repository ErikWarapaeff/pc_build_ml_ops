#!/usr/bin/env python3
# type: ignore
# ruff: noqa: E402
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ
—Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∏–∑ DVC.
"""

import os
import sys
from pathlib import Path

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—É—Ç–∏ –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ –¥—Ä—É–≥–∏—Ö –º–æ–¥—É–ª–µ–π –ø—Ä–æ–µ–∫—Ç–∞
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# –ò–º–ø–æ—Ä—Ç—ã –ø–æ—Å–ª–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ sys.path
import argparse
import json  # –î–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
import logging
import time
from typing import Any

import mlflow
import yaml
from dvc.repo import Repo

from src.chat_backend import ChatBot  # type: ignore
from src.load_config import LoadConfig

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler(sys.stdout)],
)
logger = logging.getLogger(__name__)

# ---------------------------------------------------------------------------
# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è: –≤—Å—Ç–∞–≤–ª—è–µ—Ç/–æ–±–Ω–æ–≤–ª—è–µ—Ç —Å–µ–∫—Ü–∏—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ README.md
# ---------------------------------------------------------------------------


def _update_readme_with_summary(table_md: str) -> None:  # pragma: no cover
    """–û–±–Ω–æ–≤–ª—è–µ—Ç –∏–ª–∏ –≤—Å—Ç–∞–≤–ª—è–µ—Ç —Å–µ–∫—Ü–∏—é —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –±–µ–Ω—á–º–∞—Ä–∫–∞ –≤ README.md.

    –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –º–∞—Ä–∫–µ—Ä—ã <!-- EVAL_RESULTS_START --> –∏ <!-- EVAL_RESULTS_END -->.
    """
    readme = Path("README.md")
    if not readme.exists():
        return

    start_marker = "<!-- EVAL_RESULTS_START -->"
    end_marker = "<!-- EVAL_RESULTS_END -->"

    content = readme.read_text(encoding="utf-8")
    section = (
        f"{start_marker}\n\n"
        f"## üìä –ü–æ—Å–ª–µ–¥–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π (–æ–±–Ω–æ–≤–ª–µ–Ω–æ {time.strftime('%Y-%m-%d %H:%M:%S')})\n\n"
        f"{table_md}\n\n"
        f"{end_marker}"
    )

    if start_marker in content and end_marker in content:
        pre = content.split(start_marker)[0]
        post = content.split(end_marker)[-1]
        new_content = pre + section + post
    else:
        new_content = content.rstrip() + "\n\n" + section + "\n"

    readme.write_text(new_content, encoding="utf-8")


class ModelEvaluator:
    """–ö–ª–∞—Å—Å –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ"""

    def __init__(self, config_path: str, use_api: bool = False):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ü–µ–Ω—â–∏–∫–∞ –º–æ–¥–µ–ª–µ–π

        Args:
            config_path: –ü—É—Ç—å –∫ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É
            use_api: –§–ª–∞–≥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è HTTP-API –¥–ª—è –æ–ø—Ä–æ—Å–∞ –º–æ–¥–µ–ª–µ–π
        """
        self.config_path = config_path
        self.original_config = self._load_config()
        self.test_questions = self._prepare_test_questions()
        self.use_api = use_api  # True ‚Üí –æ–ø—Ä–∞—à–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ HTTP-API
        # –ü–æ–¥—Ç—è–≥–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ DVC –ø–µ—Ä–µ–¥ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º (–µ—Å–ª–∏ —Ä–∞–±–æ—Ç–∞–µ–º –ª–æ–∫–∞–ª—å–Ω–æ)
        if not self.use_api:
            Repo().pull()

    def _load_config(self) -> dict[str, Any]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–∑ —Ñ–∞–π–ª–∞"""
        if not os.path.exists(self.config_path):
            logger.error(f"–§–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {self.config_path}")
            sys.exit(1)

        try:
            with open(self.config_path, encoding="utf-8") as f:
                config = yaml.safe_load(f)
            # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ç–∏–ø—É Dict[str, Any]
            if not isinstance(config, dict):
                config = {}
            return config  # type: ignore[no-any-return]
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {str(e)}")
            sys.exit(1)

    def _save_config(self, config: dict[str, Any]) -> None:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –≤ —Ñ–∞–π–ª"""
        try:
            with open(self.config_path, "w", encoding="utf-8") as f:
                yaml.dump(config, f, default_flow_style=False)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {str(e)}")

    def _prepare_test_questions(self) -> list[str]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–ø–∏—Å–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π"""
        return [
            "–ü—Ä–∏–≤–µ—Ç, —á—Ç–æ —Ç—ã —É–º–µ–µ—à—å?",
            "–ê –∫–∞–∫–∞—è —Å—Ä–µ–¥–Ω—è—è —Ü–µ–Ω–∞ –Ω–∞ –≤–∏–¥–µ–æ–∫–∞—Ä—Ç—ã —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π 4–∫?",
            "–ú–Ω–µ –Ω—É–∂–µ–Ω –∏–≥—Ä–æ–≤–æ–π –ü–ö —á—Ç–æ–±—ã –≤–∏–¥–µ–æ–∫–∞—Ä—Ç–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–ª–∞ 4–∫, –º–æ–π –±—é–¥–∂–µ—Ç 200–∫",
            "–ê –Ω–∞—Å–∫–æ–ª—å–∫–æ –≤ –¥–∞–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç –≤–∏–¥–µ–æ–∫–∞—Ä—Ç—É?",
            "–•–æ—Ä–æ—à–æ, –∞ –ø–æ–π–¥–µ—Ç –ª–∏ –Ω–∞ –¥–∞–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ –∏–≥—Ä–∞ Cyberpunk 2077?",
            "–•–æ—Ä–æ—à–æ, –Ω–∞–π–¥–∏ –º–Ω–µ —Ç–æ–≥–¥–∞ –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ —Ü–µ–Ω—ã –Ω–∞ –¥–∞–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É.",
        ]

    # --- –ù–æ–≤—ã–π –±–ª–æ–∫: –∑–∞–ø—Ä–æ—Å –∫ –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ HTTP-API ---
    MODEL_API_PORTS: dict[str, int] = {
        "gpt-3.5-turbo": 8001,
        "gpt-4o": 8002,
        "gpt-4o-mini": 8003,
    }

    def _predict_via_api(self, model_name: str, prompt: str) -> tuple[str, float]:
        """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å –∫ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—É –º–æ–¥–µ–ª–∏ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç (–æ—Ç–≤–µ—Ç, –≤—Ä–µ–º—è)."""

        import requests  # –ª–æ–∫–∞–ª—å–Ω—ã–π –∏–º–ø–æ—Ä—Ç, —á—Ç–æ–±—ã –Ω–µ –¥–æ–±–∞–≤–ª—è—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –ø—Ä–∏ –æ—Ñ—Ñ–ª–∞–π–Ω-—Ä–µ–∂–∏–º–µ

        port = self.MODEL_API_PORTS.get(model_name)
        if port is None:
            raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø–æ—Ä—Ç –¥–ª—è –º–æ–¥–µ–ª–∏ {model_name}")

        url = f"http://localhost:{port}/predict"
        start = time.time()
        resp = requests.post(url, json={"prompt": prompt})
        duration = time.time() - start

        if resp.status_code != 200:
            raise RuntimeError(f"API {model_name} –≤–µ—Ä–Ω—É–ª {resp.status_code}: {resp.text[:200]}")

        try:
            answer = resp.json().get("answer", "")
        except ValueError:
            answer = resp.text
        return answer, duration

    def test_model(self, model_name: str) -> dict[str, Any]:
        """
        –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–∞—Ö

        Args:
            model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        """
        if not self.use_api:
            # –ú–µ–Ω—è–µ–º –º–æ–¥–µ–ª—å –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            config = self.original_config.copy()
            config["openai_models"]["model"] = model_name
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ–≤—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º
            self._save_config(config)
            LoadConfig()
            chatbot: ChatBot | None = ChatBot()
        else:
            chatbot = None  # noqa: E501

        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        results: dict[str, Any] = {
            "model": model_name,
            "responses": [],
            "timings": [],
            "total_time": 0,
        }

        # –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
        logger.info(f"–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏: {model_name}")
        start_time_total = time.time()

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º List –¥–ª—è chat_history
        chat_history: list[tuple[str | None, str]] = []

        for i, question in enumerate(self.test_questions):
            logger.info(f"–í–æ–ø—Ä–æ—Å {i+1}: {question}")

            # –ó–∞–º–µ—Ä—è–µ–º –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞
            start_time = time.time()

            # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç –±–æ—Ç–∞
            try:
                if self.use_api:
                    bot_response, response_time = self._predict_via_api(model_name, question)
                else:
                    _, chat_history_new, _ = chatbot.respond(chat_history, question)
                    chat_history = chat_history_new
                    bot_response = (
                        chat_history[-1].get("content")
                        if isinstance(chat_history[-1], dict)
                        else None
                    ) or "–ù–µ—Ç –æ—Ç–≤–µ—Ç–∞"
                    response_time = time.time() - start_time
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –æ—Ç–≤–µ—Ç–∞: {str(e)}")
                bot_response = f"–û–®–ò–ë–ö–ê: {str(e)}"
                response_time = time.time() - start_time

            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            if isinstance(results["responses"], list):
                results["responses"].append(
                    {"question": question, "response": bot_response, "time": response_time}
                )

            if isinstance(results["timings"], list):
                results["timings"].append(response_time)

            logger.info(f"–û—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω –∑–∞ {response_time:.2f} —Å–µ–∫—É–Ω–¥")

        end_time_total = time.time()
        total_time = end_time_total - start_time_total
        results["total_time"] = total_time

        logger.info(f"–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ {model_name} –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {total_time:.2f} —Å–µ–∫—É–Ω–¥")

        if not self.use_api:
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            self._save_config(self.original_config)
            LoadConfig()

        return results

    def evaluate_models(self, models: list[str]) -> dict[str, Any]:
        """
        –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

        Args:
            models: –°–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
        """
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ MLflow
        mlflow.set_experiment("model_evaluation")
        evaluation_results: dict[str, Any] = {
            "models": [],
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        }

        for model_name in models:
            with mlflow.start_run(run_name=model_name):
                logger.info(f"–ù–∞—á–∞–ª–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏: {model_name}")
                # –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –∏ –ø–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                model_results = self.test_model(model_name)
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–∞–∫ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç
                artifact_file = f"model_results_{model_name}.json"
                with open(artifact_file, "w", encoding="utf-8") as f:
                    json.dump(model_results, f, ensure_ascii=False, indent=2)
                mlflow.log_artifact(artifact_file)
                # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –º–µ—Ç—Ä–∏–∫
                mlflow.log_param("model_name", model_name)
                for idx, t in enumerate(model_results["timings"]):
                    mlflow.log_metric("response_time", t, step=idx)
                mlflow.log_metric("total_time", model_results["total_time"])
            # –ö–æ–Ω–µ—Ü MLflow –∑–∞–ø—É—Å–∫–∞

            if isinstance(evaluation_results["models"], list):
                evaluation_results["models"].append(model_results)

            if not self.use_api:
                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –º–µ–∂–¥—É —Ç–µ—Å—Ç–∞–º–∏ –º–æ–¥–µ–ª–µ–π
                self._save_config(self.original_config)
                from src.load_config import LoadConfig

                LoadConfig()

            logger.info(f"–ó–∞–≤–µ—Ä—à–µ–Ω–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏: {model_name}")

            # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–µ–π
            time.sleep(1)

        if not self.use_api:
            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∏—Å—Ö–æ–¥–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –≤ –∫–æ–Ω—Ü–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            self._save_config(self.original_config)
            from src.load_config import LoadConfig

            LoadConfig()

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self._save_evaluation_results(evaluation_results)
        self._save_summary(evaluation_results)

        return evaluation_results

    def _save_evaluation_results(self, results: dict[str, Any]) -> None:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤ —Ñ–∞–π–ª"""
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        results_dir = os.path.join(
            os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "evaluation_results"
        )

        os.makedirs(results_dir, exist_ok=True)

        results_path = os.path.join(results_dir, f"model_evaluation_{timestamp}.yml")

        try:
            with open(results_path, "w", encoding="utf-8") as f:
                yaml.dump(results, f, default_flow_style=False)
            logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {results_path}")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {str(e)}")

    def _save_summary(self, evaluation_results: dict[str, Any]) -> None:
        """–§–æ—Ä–º–∏—Ä—É–µ—Ç –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –ø–æ —Å—Ä–µ–¥–Ω–µ–º—É/–æ–±—â–µ–º—É –≤—Ä–µ–º–µ–Ω–∏ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç CSV –∏ Markdown."""

        import pandas as pd  # –ª–æ–∫–∞–ª—å–Ω—ã–π –∏–º–ø–æ—Ä—Ç

        models_data = evaluation_results.get("models", [])
        if not isinstance(models_data, list):
            return

        rows: list[dict[str, Any]] = []
        for m in models_data:
            timings = m.get("timings", [])
            if not timings:
                continue

            # --- –ù–æ–≤–∞—è –º–µ—Ç—Ä–∏–∫–∞: —Å—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞ (–≤ —Å–ª–æ–≤–∞—Ö) ---
            responses = m.get("responses", [])
            lengths = [
                len(str(r.get("response", "")).split()) for r in responses if isinstance(r, dict)
            ]
            avg_len = sum(lengths) / len(lengths) if lengths else 0

            rows.append(
                {
                    "model": m.get("model"),
                    "avg_time_s": sum(timings) / len(timings),
                    "total_time_s": m.get("total_time", 0),
                    "avg_length_words": avg_len,
                }
            )

        if not rows:
            return

        df = pd.DataFrame(rows).sort_values("avg_time_s")

        ts = time.strftime("%Y%m%d_%H%M%S")
        out_dir = Path(__file__).resolve().parent.parent / "evaluation_results"
        out_dir.mkdir(exist_ok=True, parents=True)

        csv_path = out_dir / f"evaluation_summary_{ts}.csv"
        md_path = out_dir / f"evaluation_summary_{ts}.md"

        df.to_csv(csv_path, index=False, float_format="%.3f")

        md = (
            "| –ú–æ–¥–µ–ª—å | –°—Ä. –≤—Ä–µ–º—è (—Å) | –û–±—â–µ–µ –≤—Ä–µ–º—è (—Å) | –°—Ä. –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞ (—Å–ª–æ–≤) |\n"
            "|-------|--------------|----------------|------------------------|\n"
        )
        for _, row in df.iterrows():
            md += (
                f"| {row['model']} | {row['avg_time_s']:.2f} | "
                f"{row['total_time_s']:.2f} | {row['avg_length_words']:.1f} |\n"
            )
        md_path.write_text(md, encoding="utf-8")

        logger.info("–°–≤–æ–¥–Ω—ã–π —Ñ–∞–π–ª —Å–æ—Ö—Ä–∞–Ω—ë–Ω: %s", csv_path)

        # –û–±–Ω–æ–≤–ª—è–µ–º README.md –∞–∫—Ç—É–∞–ª—å–Ω—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        _update_readme_with_summary(md)


def main() -> None:
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π"""
    parser = argparse.ArgumentParser(description="–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –≤ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ")
    parser.add_argument(
        "--config",
        type=str,
        default="configs/config.yml",
        help="–ü—É—Ç—å –∫ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: configs/config.yml)",
    )
    parser.add_argument(
        "--models",
        type=str,
        nargs="+",
        default=["gpt-4o-mini", "gpt-4o", "gpt-3.5-turbo"],
        help="–°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: gpt-4o-mini gpt-4o gpt-3.5-turbo)",
    )
    parser.add_argument(
        "--database",
        type=str,
        default="data/databases/pc_accessories_2.db",
        help="–ü—É—Ç—å –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: data/databases/pc_accessories_2.db)",
    )
    parser.add_argument(
        "--log-level",
        type=str,
        default="INFO",
        choices=["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"],
        help="–£—Ä–æ–≤–µ–Ω—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: INFO)",
    )
    parser.add_argument(
        "--use-api",
        action="store_true",
        help="–ó–∞–ø—Ä–∞—à–∏–≤–∞—Ç—å –º–æ–¥–µ–ª–∏ –ø–æ HTTP-API –≤–º–µ—Å—Ç–æ –ø—Ä—è–º–æ–≥–æ –≤—ã–∑–æ–≤–∞ ChatBot",
    )

    args = parser.parse_args()

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —É—Ä–æ–≤–Ω—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    logger.setLevel(getattr(logging, args.log_level))

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
    if not os.path.exists(args.database):
        logger.error(f"–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {args.database}")
        sys.exit(1)

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
    if not os.path.exists(args.config):
        logger.error(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {args.config}")
        sys.exit(1)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ü–µ–Ω—â–∏–∫–∞ –º–æ–¥–µ–ª–µ–π
    evaluator = ModelEvaluator(args.config, use_api=args.use_api)

    # –ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π
    logger.info(f"–ù–∞—á–∞–ª–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π: {', '.join(args.models)}")
    results = evaluator.evaluate_models(args.models)

    # –í—ã–≤–æ–¥–∏–º –∫—Ä–∞—Ç–∫—É—é —Å–≤–æ–¥–∫—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    logger.info("–°–≤–æ–¥–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:")
    for model_result in results["models"]:
        model_name = model_result["model"]
        total_time = model_result["total_time"]
        avg_time = sum(model_result["timings"]) / len(model_result["timings"])

        logger.info(f"–ú–æ–¥–µ–ª—å: {model_name}")
        logger.info(f"  –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time:.2f} —Å–µ–∫—É–Ω–¥")
        logger.info(f"  –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞: {avg_time:.2f} —Å–µ–∫—É–Ω–¥")

    logger.info("–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –∑–∞–≤–µ—Ä—à–µ–Ω–æ")


if __name__ == "__main__":
    main()

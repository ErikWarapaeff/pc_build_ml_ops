#!/usr/bin/env python3
"""–ë—ã—Å—Ç—Ä—ã–π –±–µ–Ω—á–º–∞—Ä–∫ HTTP-API —Å–µ—Ä–≤–∏—Å–æ–≤ –º–æ–¥–µ–ª–µ–π.

‚Ä¢ –ò–∑–º–µ—Ä—è–µ—Ç –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞ (latency) –∏ –¥–ª–∏–Ω—É —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.
‚Ä¢ –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–æ–¥—Ä–æ–±–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (JSON) –∏ —Å–≤–æ–¥–Ω—É—é —Ç–∞–±–ª–∏—Ü—É (CSV, Markdown).
"""
from __future__ import annotations

import argparse
import json
import sys
import time
from pathlib import Path
from typing import Any

import pandas as pd
import requests

# –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –º–æ–¥–µ–ª—å ‚Üí –ø–æ—Ä—Ç –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞ FastAPI
MODEL_API_PORTS: dict[str, int] = {
    "gpt-3.5-turbo": 8001,
    "gpt-4o": 8002,
    "gpt-4o-mini": 8003,
}

# –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –Ω–∞–±–æ—Ä –≤–æ–ø—Ä–æ—Å–æ–≤ (–º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å, –ø–µ—Ä–µ–¥–∞–≤ --dataset)
DEFAULT_QUESTIONS: list[str] = [
    "–ü—Ä–∏–≤–µ—Ç, —á—Ç–æ —Ç—ã —É–º–µ–µ—à—å?",
    "–ê –∫–∞–∫–∞—è —Å—Ä–µ–¥–Ω—è—è —Ü–µ–Ω–∞ –Ω–∞ –≤–∏–¥–µ–æ–∫–∞—Ä—Ç—ã —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π 4–∫?",
    "–ú–Ω–µ –Ω—É–∂–µ–Ω –∏–≥—Ä–æ–≤–æ–π –ü–ö —á—Ç–æ–±—ã –≤–∏–¥–µ–æ–∫–∞—Ä—Ç–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–ª–∞ 4–∫, –º–æ–π –±—é–¥–∂–µ—Ç 200–∫",
    "–ê –Ω–∞—Å–∫–æ–ª—å–∫–æ –≤ –¥–∞–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç –≤–∏–¥–µ–æ–∫–∞—Ä—Ç—É?",
    "–•–æ—Ä–æ—à–æ, –∞ –ø–æ–π–¥–µ—Ç –ª–∏ –Ω–∞ –¥–∞–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ –∏–≥—Ä–∞ Cyberpunk 2077?",
    "–•–æ—Ä–æ—à–æ, –Ω–∞–π–¥–∏ –º–Ω–µ —Ç–æ–≥–¥–∞ –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ —Ü–µ–Ω—ã –Ω–∞ –¥–∞–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É.",
]


def load_questions(path: str | None) -> list[str]:
    if path is None:
        return DEFAULT_QUESTIONS
    p = Path(path)
    if not p.exists():
        print(f"–§–∞–π–ª —Å –¥–∞—Ç–∞—Å–µ—Ç–æ–º –Ω–µ –Ω–∞–π–¥–µ–Ω: {p}", file=sys.stderr)
        sys.exit(1)
    if p.suffix in {".txt"}:
        return [line.strip() for line in p.read_text(encoding="utf-8").splitlines() if line.strip()]
    if p.suffix in {".json"}:
        data = json.loads(p.read_text(encoding="utf-8"))
        if isinstance(data, list):
            return [str(x) for x in data]
    print("–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç–∞—Å–µ—Ç–∞", file=sys.stderr)
    sys.exit(1)


def benchmark_model(model_name: str, questions: list[str]) -> dict[str, Any]:
    port = MODEL_API_PORTS.get(model_name)
    if port is None:
        raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø–æ—Ä—Ç –¥–ª—è –º–æ–¥–µ–ª–∏ {model_name}")
    url = f"http://localhost:{port}/predict"

    responses: list[dict[str, Any]] = []
    timings: list[float] = []
    lengths: list[int] = []  # –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞ –≤ —Å–ª–æ–≤–∞—Ö

    for q in questions:
        start = time.perf_counter()
        resp = requests.post(url, json={"prompt": q})
        duration = time.perf_counter() - start
        timings.append(duration)

        if resp.status_code != 200:
            answer = f"ERROR {resp.status_code}: {resp.text[:200]}"
        else:
            try:
                answer = resp.json().get("answer", "")
            except ValueError:
                answer = resp.text

        # –í—ã—á–∏—Å–ª—è–µ–º –¥–ª–∏–Ω—É –æ—Ç–≤–µ—Ç–∞ –≤ —Å–ª–æ–≤–∞—Ö –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º
        lengths.append(len(str(answer).split()))
        responses.append(
            {"question": q, "answer": answer, "time": duration, "length": len(answer.split())}
        )

    return {
        "model": model_name,
        "responses": responses,
        "timings": timings,
        "avg_length": sum(lengths) / len(lengths) if lengths else 0,
        "avg_time": sum(timings) / len(timings),
        "total_time": sum(timings),
    }


def save_results(results: list[dict[str, Any]], out_dir: Path) -> None:
    out_dir.mkdir(parents=True, exist_ok=True)
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    raw_path = out_dir / f"api_benchmark_results_{timestamp}.json"
    summary_path_csv = out_dir / f"api_benchmark_summary_{timestamp}.csv"
    summary_path_md = out_dir / f"api_benchmark_summary_{timestamp}.md"

    # RAW
    json.dump(results, raw_path.open("w", encoding="utf-8"), ensure_ascii=False, indent=2)

    # SUMMARY
    rows = [
        {
            "model": r["model"],
            "avg_time_s": r["avg_time"],
            "total_time_s": r["total_time"],
            "avg_length_words": r.get("avg_length", 0),
        }
        for r in results
    ]
    df = pd.DataFrame(rows).sort_values("avg_time_s")
    df.to_csv(summary_path_csv, index=False, float_format="%.3f")

    # Markdown
    md = (
        "| –ú–æ–¥–µ–ª—å | –°—Ä. –≤—Ä–µ–º—è (—Å) | –û–±—â–µ–µ –≤—Ä–µ–º—è (—Å) | –°—Ä. –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞ (—Å–ª–æ–≤) |\n"
        "|-------|--------------|----------------|------------------------|\n"
    )
    for _, row in df.iterrows():
        md += (
            f"| {row['model']} | {row['avg_time_s']:.2f} | "
            f"{row['total_time_s']:.2f} | {row['avg_length_words']:.1f} |\n"
        )
    summary_path_md.write_text(md, encoding="utf-8")

    # --- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ README.md ---
    _update_readme_with_benchmark(md)

    print(
        f"‚úî –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:\n  RAW: {raw_path}\n  CSV: {summary_path_csv}\n  MD : {summary_path_md}"
    )


def main() -> None:
    parser = argparse.ArgumentParser(description="Benchmark HTTP-API –º–æ–¥–µ–ª–µ–π")
    parser.add_argument(
        "--models", nargs="+", default=list(MODEL_API_PORTS.keys()), help="–°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π"
    )
    parser.add_argument("--dataset", type=str, help="–§–∞–π–ª —Å –≤–æ–ø—Ä–æ—Å–∞–º–∏ (.txt –∏–ª–∏ .json)")
    parser.add_argument(
        "--out", type=str, default="reports", help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á—ë—Ç–æ–≤"
    )
    args = parser.parse_args()

    questions = load_questions(args.dataset)

    all_results: list[dict[str, Any]] = []
    for model_name in args.models:
        print(f"‚Üí –¢–µ—Å—Ç–∏—Ä—É–µ–º {model_name} ...")
        res = benchmark_model(model_name, questions)
        print(f"  Avg time: {res['avg_time']:.2f}s, total: {res['total_time']:.2f}s")
        all_results.append(res)

    save_results(all_results, Path(args.out))


if __name__ == "__main__":
    main()


# ---------------------------------------------------------------------------
# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è util-—Ñ—É–Ω–∫—Ü–∏—è: –≤—Å—Ç–∞–≤–ª—è–µ—Ç/–æ–±–Ω–æ–≤–ª—è–µ—Ç —Å–µ–∫—Ü–∏—é –±–µ–Ω—á–º–∞—Ä–∫–∞ –≤ README.md
# ---------------------------------------------------------------------------


def _update_readme_with_benchmark(table_md: str) -> None:  # pragma: no cover
    """–î–æ–±–∞–≤–ª—è–µ—Ç/–æ–±–Ω–æ–≤–ª—è–µ—Ç —Å–µ–∫—Ü–∏—é —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –±–µ–Ω—á–º–∞—Ä–∫–∞ –≤ README.md.

    –ò—â–µ—Ç –º–∞—Ä–∫–µ—Ä—ã `<!-- BENCHMARK_RESULTS_START -->` –∏ `<!-- BENCHMARK_RESULTS_END -->`.
    –ï—Å–ª–∏ –æ–Ω–∏ —Å—É—â–µ—Å—Ç–≤—É—é—Ç ‚Äî –∑–∞–º–µ–Ω—è–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –º–µ–∂–¥—É –Ω–∏–º–∏. –ï—Å–ª–∏ –Ω–µ—Ç ‚Äî –¥–æ–±–∞–≤–ª—è–µ—Ç
    –Ω–æ–≤—É—é —Å–µ–∫—Ü–∏—é –≤ –∫–æ–Ω–µ—Ü README.md.
    """

    readme_path = Path("README.md")
    if not readme_path.exists():
        return

    start_marker = "<!-- BENCHMARK_RESULTS_START -->"
    end_marker = "<!-- BENCHMARK_RESULTS_END -->"

    content = readme_path.read_text(encoding="utf-8")

    section_md = (
        f"{start_marker}\n\n"
        f"## üöÄ –ü–æ—Å–ª–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –º–æ–¥–µ–ª–µ–π (–æ–±–Ω–æ–≤–ª–µ–Ω–æ {time.strftime('%Y-%m-%d %H:%M:%S')})\n\n"
        f"{table_md}\n\n"
        f"{end_marker}"
    )

    if start_marker in content and end_marker in content:
        # –ó–∞–º–µ–Ω—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é —Å–µ–∫—Ü–∏—é
        pre = content.split(start_marker)[0]
        post = content.split(end_marker)[-1]
        new_content = pre + section_md + post
    else:
        # –ü—Ä–æ—Å—Ç–æ –¥–æ–±–∞–≤–ª—è–µ–º –≤ –∫–æ–Ω–µ—Ü
        new_content = content.rstrip() + "\n\n" + section_md + "\n"

    readme_path.write_text(new_content, encoding="utf-8")

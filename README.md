# PC-Build.AI
Advanced multi-agent application designed to automate and optimize the process of custom PC building

## Системные требования

- Python 3.12 или выше
- Poetry для управления зависимостями
- Docker (опционально, для контейнеризации)

## Инструкции для запуска:
1. **Склонируйте репозиторий:**
   ```bash
   git clone https://github.com/ErikWarapaeff/PC-Build.AI
   ```

2. **Установите Poetry:**
   - **Windows (PowerShell):**
     ```powershell
     (Invoke-WebRequest -Uri https://install.python-poetry.org -UseBasicParsing).Content | python -
     ```
   - **macOS/Linux:**
     ```bash
     curl -sSL https://install.python-poetry.org | python3 -
     ```

3. **Установите зависимости и активируйте виртуальное окружение:**
   ```bash
   poetry install
   poetry shell
   ```

4. **Введите все API ключи:**
   Введите все ключи в  `.env`:
   ```
   OPEN_AI_API_KEY=...
   LANGCHAIN_API_KEY=...
   ```

5. **Запустите приложение на градио:**
   ```bash
   poetry run python src/app.py
   ```

6. **Пользовательские настройки:**
   Измените `config/config.yml` если нужно.


7. **Примеры запросов**

- Привет, что ты умеешь?
- А какая средняя цена на видеокарты с поддержкой 4к?
- Мне нужен игровой ПК чтобы видеокарта поддерживала 4к, мой бюджет 200к
- А насколько в данной системе процессор раскрывает видеокарту?
- Хорошо, а пойдет ли на данной системе игра Cyberpunk 2077?
- Хорошо, найди мне тогда актуальные цены на данную систему.


## Система тестирования и сравнения языковых моделей

Проект содержит инструменты для тестирования производительности и эффективности различных языковых моделей.
Эти инструменты позволяют оценить скорость ответа, использование токенов и качество ответов для разных моделей.

### Основные инструменты

1. **Тестирование моделей** (model_evaluator.py):
   ```bash
   python src/model_evaluator.py --models gpt-4o-mini gpt-4o gpt-3.5-turbo
   ```
   Запускает тестирование указанных моделей на стандартном наборе вопросов, измеряет время ответа и
   сохраняет результаты для дальнейшего анализа.

2. **Анализ использования токенов** (token_counter.py):
   ```bash
   python src/token_counter.py --results-file path/to/evaluation_results.yml
   ```
   Анализирует использование токенов различными моделями и рассчитывает метрики эффективности,
   такие как токены в секунду и соотношение входных/выходных токенов.

3. **Визуализация результатов** (visualization.py):
   ```bash
   python src/visualization.py --results-file path/to/evaluation_results.yml
   ```
   Создает наглядные визуализации результатов тестирования, включая графики времени ответа,
   и генерирует базовый HTML-отчет.

4. **Полные отчеты сравнения** (generate_report.py):
   ```bash
   # Запуск тестирования и генерация полного отчета
   python src/generate_report.py --run-tests --models gpt-4o-mini gpt-4o gpt-3.5-turbo

   # Генерация отчета из существующих результатов
   python src/generate_report.py
   ```
   Комбинирует данные из всех инструментов и создает комплексный HTML-отчет с рекомендациями по выбору оптимальной модели.

5. **Единый скрипт для полного цикла тестирования** (run_complete_test.py):
   ```bash
   python src/run_complete_test.py --models gpt-4o-mini gpt-4o gpt-3.5-turbo
   ```
   Выполняет полный цикл тестирования в один запуск — последовательно запускает тестирование моделей,
   анализ токенов и генерацию отчета с мониторингом выполнения каждого этапа.

6. **Валидация баз данных под контролем DVC** (db_validator.py):
   ```bash
   # Проверка целостности базы данных с генерацией полного отчета
   python src/db_validator.py --db-path data/databases/pc_accessories_2.db --detailed

   # Базовая проверка с выводом только в JSON формате
   python src/db_validator.py --format json
   ```
   Проверяет целостность и качество данных в базе данных, контролируемой DVC:
   - Анализирует структуру таблиц и столбцов
   - Проверяет ограничения primary key и not null
   - Выявляет дубликаты и пропущенные значения
   - Анализирует диапазоны данных и статистику по каждому столбцу
   - Проверяет согласованность с версией в DVC

### Возможности отчетов

Сгенерированные отчеты содержат:
- Сравнительные таблицы производительности моделей
- Графики скорости ответа и эффективности использования токенов
- Подробные рекомендации по выбору модели для разных задач
- Детальный анализ ответов на каждый тестовый вопрос
- Статистику по использованию токенов и времени ответа

### Пример использования

```bash
# Простой запуск полного цикла тестирования с параметрами по умолчанию:
python src/run_complete_test.py

# Чтобы протестировать только конкретные модели:
python src/run_complete_test.py --models gpt-4o-mini gpt-4o

# Чтобы указать нестандартные пути к конфигурации и результатам:
python src/run_complete_test.py --config configs/custom_config.yml --results-dir custom_results --output-dir reports

# Проверка целостности базы данных и её статуса в DVC:
python src/db_validator.py --db-path data/databases/pc_accessories_2.db --detailed
```

Результаты тестирования и отчеты сохраняются в директории `evaluation_results/`.
Результаты валидации баз данных сохраняются в директории `validation_results/`.

**Агентный граф:**

!['граф'](image.png)

## Инструкции для разработчиков

### Управление зависимостями с Poetry

Проект использует [Poetry](https://python-poetry.org/) для управления зависимостями и создания воспроизводимых сборок.

1. **Добавление новой зависимости:**
   ```bash
   poetry add package-name
   ```

2. **Добавление dev-зависимости:**
   ```bash
   poetry add --group dev package-name
   ```

3. **Обновление зависимостей:**
   ```bash
   poetry update
   ```

4. **Экспорт зависимостей в requirements.txt (если необходимо):**
   ```bash
   poetry export -f requirements.txt --output requirements.txt
   ```

5. **Создание сборки проекта:**
   ```bash
   poetry build
   ```

### Настройка линтеров и форматеров

1. **Установите инструменты для разработки:**
   ```bash
   poetry install --with dev
   ```

2. **Настройте pre-commit хуки:**
   ```bash
   poetry run pre-commit install
   ```

3. **Запуск проверок вручную:**
   ```bash
   # Запуск всех pre-commit хуков
   poetry run pre-commit run --all-files

   # Запуск Black форматирования
   poetry run black .

   # Запуск Ruff линтера
   poetry run ruff check .

   # Запуск isort для сортировки импортов
   poetry run isort .

   # Запуск mypy для проверки типов
   poetry run mypy src/ app.py
   ```

### Автоматические проверки

При создании Pull Request или push в ветку `main` автоматически запускаются линтеры и тесты через GitHub Actions.
Результаты проверок можно увидеть на странице Pull Request или во вкладке Actions в репозитории.

## CI/CD Пайплайн

Этот проект использует комплексный CI/CD пайплайн, управляемый с помощью GitHub Actions. Пайплайн автоматизирует различные задачи разработки и развертывания, обеспечивая качество кода, согласованность и эффективную доставку артефактов.

Пайплайн запускается при следующих событиях:
*   **Push** в ветку `main`.
*   **Push** тегов, соответствующих шаблону `v*` (например, `v1.0.0`, `v0.1.2`).
*   **Pull Request**, нацеленный на ветку `main`.

Пайплайн состоит из следующих основных задач (jobs):

### 1. Проверка Кода (`lint`)
Эта задача отвечает за поддержание качества и согласованности кода.
*   **Триггеры:** Запускается при каждом push в `main`, pull request в `main` и для тегов `v*`.
*   **Шаги:**
    1.  **Checkout code (Клонирование кода):** Получает последнюю версию репозитория.
    2.  **Set up Python (Настройка Python):** Инициализирует окружение Python 3.12.
    3.  **Install Poetry (Установка Poetry):** Устанавливает менеджер зависимостей Poetry.
    4.  **Configure Poetry (Конфигурация Poetry):** Настраивает Poetry для создания виртуальных окружений внутри проекта.
    5.  **Install dependencies (Установка зависимостей):** Устанавливает зависимости проекта, включая инструменты для разработки и `types-PyYAML` для проверки типов.
    6.  **Run black (Запуск black):** Проверяет форматирование кода Python с использованием Black.
    7.  **Run isort (Запуск isort):** Проверяет и сортирует импорты с использованием isort.
    8.  **Run ruff (Запуск ruff):** Выполняет линтинг с использованием Ruff для широкого спектра проверок стиля кода Python и поиска ошибок.
    9.  **Run mypy (Запуск mypy):** Проводит статическую проверку типов с использованием Mypy для выявления ошибок типизации. Конфигурация Mypy настроена на менее строгий режим, игнорируя некоторые распространенные ошибки типизации для облегчения интеграции и позволяя постепенно улучшать аннотации типов.

### 2. Выполнение Тестов (`test`)
Эта задача выполняет автоматизированные тесты для обеспечения корректности и стабильности приложения.
*   **Триггеры:** Запускается после успешного завершения задачи `lint`, при push в `main`, pull request в `main` и для тегов `v*`.
*   **Шаги:**
    1.  **Checkout code.**
    2.  **Set up Python 3.12.**
    3.  **Install Poetry.**
    4.  **Configure Poetry.**
    5.  **Install dependencies** (включая зависимости для разработки).
    6.  **Run tests (Запуск тестов):** Выполняет тесты с использованием `pytest` и генерирует отчет о покрытии кода (`coverage.xml`).
    7.  **Upload coverage to Codecov (Загрузка покрытия в Codecov):** Отправляет отчет о покрытии в Codecov для анализа и отслеживания (требуется секрет `CODECOV_TOKEN`).

### 3. Сборка Python Пакета (`build-package`)
Эта задача собирает дистрибутивный пакет Python (wheel и sdist).
*   **Триггеры:** Запускается **только для тегов `v*`** после успешного завершения задачи `test`. Это гарантирует, что пакеты собираются только для официальных релизов.
*   **Шаги:**
    1.  **Checkout code.**
    2.  **Set up Python 3.12.**
    3.  **Install Poetry.**
    4.  **Build package (Сборка пакета):** Создает дистрибутивные файлы с помощью `poetry build`.
    5.  **Store version (Сохранение версии):** Извлекает версию из Git-тега (например, `v1.0.0` становится `1.0.0`) и сохраняет ее в переменной окружения.
    6.  **Upload package artifact (Загрузка артефакта пакета):** Архивирует директорию `dist/` (содержащую собранный пакет) как артефакт GitHub с именем `dist`. Артефакты хранятся 3 дня.

### 4. Сборка Docker Образа (`build-docker`)
Эта задача собирает Docker-образ для приложения.
*   **Триггеры:** Запускается после успешного завершения задачи `test`, при push в `main`, pull request в `main` и для тегов `v*`.
*   **Шаги:**
    1.  **Checkout code.**
    2.  **Set up Docker Buildx (Настройка Docker Buildx):** Инициализирует Docker Buildx для расширенной сборки образов.
    3.  **Set Docker metadata (Установка метаданных Docker):** Генерирует теги и метки для Docker-образа на основе Git-веток, тегов и SHA коммитов (например, `main`, `v1.0.0`, `sha-abcdef`).
    4.  **Build and export Docker image (Сборка и экспорт Docker-образа):** Собирает Docker-образ с использованием `Dockerfile` и загружает его локально. Образ тегируется метаданными, сгенерированными на предыдущем шаге. Собранный образ экспортируется в файл `.tar`.
    5.  **Upload Docker image artifact (Загрузка артефакта Docker-образа):** Архивирует экспортированный Docker-образ (файл `.tar`) как артефакт GitHub с именем `docker-image`. Артефакты хранятся 1 день.

### 5. Публикация Python Пакета (`publish-package`)
Эта задача публикует пакет Python на PyPI и создает Релиз на GitHub.
*   **Триггеры:** Запускается **только для тегов `v*`** после успешного завершения задачи `build-package`.
*   **Шаги:**
    1.  **Download artifact (Скачивание артефакта):** Получает артефакт `dist` (собранный пакет) из задачи `build-package`.
    2.  **Publish to PyPI (Публикация на PyPI):** Публикует пакет в Python Package Index (PyPI) с использованием действия `pypa/gh-action-pypi-publish`. Требуется секрет `PYPI_API_TOKEN` для аутентификации. Пропускает, если версия уже существует.
    3.  **Create GitHub Release (Создание Релиза GitHub):** Создает новый релиз на GitHub, используя Git-тег в качестве версии релиза. Файлы собранного пакета из `dist/` прикрепляются к релизу. Заметки к релизу ссылаются на `CHANGELOG.md`.

### 6. Публикация Docker Образа (`publish-docker`)
Эта задача публикует Docker-образ в GitHub Container Registry (ghcr.io).
*   **Триггеры:** Запускается после успешного завершения задачи `build-docker`. Выполняется при:
    *   Push в ветку `main`.
    *   Push тегов `v*`.
*   **Шаги:**
    1.  **Download artifact (Скачивание артефакта):** Получает артефакт `docker-image` (файл `.tar`) из задачи `build-docker`.
    2.  **Load Docker image (Загрузка Docker-образа):** Загружает образ из файла `.tar` в локальный Docker демон.
    3.  **Login to GitHub Container Registry (Вход в GitHub Container Registry):** Аутентифицируется в `ghcr.io` с использованием `GITHUB_TOKEN`.
    4.  **Extract Docker metadata (Извлечение метаданных Docker):** Генерирует целевые теги для `ghcr.io` (например, `ghcr.io/OWNER/REPO:main`, `ghcr.io/OWNER/REPO:v1.0.0`).
    5.  **Push Docker image (Отправка Docker-образа):** Тегирует загруженный локальный образ (используя его имя в нижнем регистре и тег версии, например, `owner/repo:main`) целевыми тегами `ghcr.io` и отправляет их в GitHub Container Registry.

### 7. Развертывание Документации (`deploy-docs`)
Эта задача собирает и развертывает документацию проекта на GitHub Pages.
*   **Триггеры:** Запускается после успешного завершения обеих задач `test` и `build-package`. Выполняется при:
    *   Push в ветку `main`.
    *   Push тегов `v*`.
*   **Шаги:**
    1.  **Checkout code.**
    2.  **Set up Python 3.12.**
    3.  **Install Poetry.**
    4.  **Install dependencies (Установка зависимостей):** Устанавливает зависимости проекта и добавляет `mkdocs-material` и `mike` (для версионированной документации).
    5.  **Build documentation (Сборка документации):** Генерирует статический сайт документации с помощью `mkdocs build`.
    6.  **Deploy to GitHub Pages (Развертывание на GitHub Pages):** Отправляет содержимое директории `site/` в ветку `gh-pages`, делая его доступным через GitHub Pages.

# Система версионирования данных для проекта PC Build ML Ops

## Описание

Этот проект использует DVC (Data Version Control) для версионирования данных. DVC позволяет отслеживать изменения в больших файлах данных, не храня их в Git.

## Структура данных

Данные хранятся в директории `data/` и включают:
- CSV-файлы с информацией о компьютерных комплектующих (CPU, GPU)
- Дополнительные CSV-файлы в директориях `csv_files/` и `csv_perifi/`
- Базы данных в директории `databases/`

## Работа с DVC

### Инициализация

Проект уже инициализирован с помощью DVC:
```bash
dvc init
```

### Локальное хранилище

Данные хранятся локально в директории `.dvc_storage/`:
```bash
dvc remote add -d local .dvc_storage
```

### Основные команды

Получить данные:
```bash
dvc pull
```

Отправить данные в хранилище:
```bash
dvc push
```

Проверить статус:
```bash
dvc status
```

### Добавление новых данных

Для добавления новых файлов данных:
```bash
dvc add data/новый_файл.csv
git add data/новый_файл.csv.dvc
git commit -m "Добавлен новый файл данных"
dvc push
```

## Интеграция MLflow

### Установка зависимостей

```bash
poetry add mlflow dvc pandas matplotlib
```

### Настройка и запуск MLflow Tracking Server

```bash
mlflow ui --backend-store-uri ./mlruns --port 5000
# Откройте http://localhost:5000 в браузере
```

### Получение данных из DVC

```bash
dvc pull
```

### Проведение экспериментов

```bash
poetry run python src/mlflow_experiments.py
```

### Сравнение результатов

```bash
poetry run python src/compare_models.py
```

### Отчет

Отчет по сравнению находится в файле `reports/comparison_report.md`. Таблица метрик сохранена в `reports/metrics_summary.csv`, а график времени инференса — в `reports/duration_comparison.png`.

## 🚀 HTTP API сервис моделей

Для интеграции с внешними системами реализован REST-сервис на базе **FastAPI** (`src/api_service.py`).

### Запуск локально

```bash
# Установите зависимости (через poetry)
poetry install --with main
# Запустите сервер
poetry run uvicorn src.api_service:app --reload --port 8000

# Проверка
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Привет, что ты умеешь?"}' | jq
```

Параметры модели задаются в `configs/api_service.yml` или через переменные окружения:

* `MODEL_NAME` — название модели OpenAI (пример: `gpt-3.5-turbo`)
* `TEMPERATURE` — температура выборки (float)
* `MAX_TOKENS` — максимальное число выходных токенов

### Docker / Docker-Compose

Для запуска нескольких версий моделей подготовлены **Dockerfile** и **docker-compose.yml**.

```bash
# Собрать и запустить весь стек
docker compose up --build -d

# Сервисы будут доступны на портах:
# 8001 — gpt-3.5-turbo
# 8002 — gpt-4o
# 8003 — gpt-4o-mini
```

### Измерение метрик

Скрипт `scripts/benchmark.py` отправляет тестовые запросы ко всем сервисам, измеряет время ответа и сохраняет результаты в `reports/`:

```bash
poetry run python scripts/benchmark.py
```

В результате появляются файлы:

* `reports/api_benchmark_results.json` — сырые результаты
* `reports/api_benchmark_results.csv` — таблица для анализа (если установлен `pandas`)

Эти данные можно использовать для дальнейшей визуализации и отчётов.

### Запуск оценки моделей через API

Каждый контейнер поддерживает эндпойнт `/evaluate`, который запускает детальный тест `model_evaluator.py` для модели данного контейнера и возвращает JSON-отчёт.

```bash
curl -X POST http://localhost:8001/evaluate | jq  # оценка gpt-3.5-turbo
```

Чтобы запустить оценку для всех контейнеров и собрать сводный отчёт:

```bash
poetry run python scripts/evaluate_containers.py
```

Результат сохранится в `reports/evaluation_overall.json`.

<!-- EVAL_RESULTS_START -->

## 📊 Последние результаты оценки моделей (обновлено 2025-06-20 00:07:23)

| Модель | Ср. время (с) | Общее время (с) | Ср. длина ответа (слов) |
|-------|--------------|----------------|------------------------|
| gpt-4o | 6.67 | 40.00 | 132.5 |
| gpt-3.5-turbo | 7.32 | 43.93 | 145.8 |
| gpt-4o-mini | 8.86 | 53.19 | 150.0 |


<!-- EVAL_RESULTS_END -->
